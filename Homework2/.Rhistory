y=min(cv_tree$dev),
colour="red",
size = 3)) +
labs(y = "Deviance", x = "Tree size") +
guides(size = "none", colour = "none") +
scale_x_discrete(labels= 1:9, limits= 1:9)
cv_tree
names(cv_tree) #  "size"   "dev"    "k"      "method"
cv_tree$size[which.min(cv_tree$dev)] # 4
opt.size <- cv_tree$size[which.min(cv_tree$dev)] # optimal size = 4
pruned_prostate <- prune.tree(tree_prostate, best = opt.size) # prune using the optimal size
# plot the pruned tree
plot(pruned_prostate)
text(pruned_prostate, pretty=0)
title(main="Pruned tree")
summary(pruned_prostate)
set.seed(1)
tree_prostate <- tree(lpsa ~ ., train_data)
pruned_prostate <- prune.tree(tree_prostate, best=4)
y_hat <- predict(pruned_prostate, test_data)
compute_mse <- function(preds, truth){mean((preds - truth)^2)}
MSE <- compute_mse(y_hat, test_data$lpsa) # calculate MSE
paste("MSE: ", MSE)
paste("Squared root of the MSE: ",sqrt(MSE))
err_matrix
set.seed(1)
nvar <- ncol(df_prostate) - 1 # number of predictors
m <- seq(1, nvar)
# initialize empty matrix that will contains the errors
err_matrix <- matrix(nrow = nvar, ncol = 2)
folds <- createFolds(df_prostate$lpsa, k = 5) # 5 folds
MSE <- c() # initialize empty list to store MSE
OOB <- c() # initialize empty list to store OOB error
for(i in m){ # iterate over m
for(f in folds){ # iterate over the folds
rf <- randomForest(lpsa ~ .,
data = df_prostate[-f, ],
mtry = i,
importance = TRUE)
y_hat_rf <- predict(rf, newdata=df_prostate[f, ])
MSE <- c(MSE, compute_mse(y_hat_rf,df_prostate[f, ]$lpsa))
OOB <- c(OOB, compute_mse(rf$predicted, df_prostate[-f,]$lpsa))
}
err_matrix[i, 1] <- mean(MSE)
err_matrix[i, 2] <- mean(OOB)
MSE <- c()
OOB <- c()
}
err_matrix
# convert matrix to df
err_df = as.data.frame(err_matrix, row.names=m)
colnames(err_df) = c("CV_error", "OOB_error")
# order results based on MSE
knitr::kable(err_df[order(err_df$CV_error),])
# order results based on OOB error
knitr::kable(err_df[order(err_df$OOB_error),])
CV_plot <- ggplot(NULL, aes(x=c(1:8), y=err_matrix[,1])) +
theme_light() +
geom_line(color="darkgrey") +
geom_point(color="black", size=3) +
geom_point(data = NULL, aes(x=c(1:8)[which.min(err_matrix[,1])],
y=min(err_matrix[,1]),
colour="red",
size = 3)) +
labs(y="CV error", x="m") +
guides(size = F, colour = F) +
scale_x_discrete(limits=1:8, labels = c(1:8))
OOB_plot <- ggplot(NULL, aes(x=c(1:8), y=err_matrix[,2])) +
theme_light() +
geom_line(color="darkgrey") +
geom_point(color="black", size=3) +
geom_point(data = NULL, aes(x=c(1:8)[which.min(err_matrix[,2])],
y=min(err_matrix[,2]),
color="red",
size = 3)) +
labs(y="OOB error", x="m") +
guides(size = F, colour = F) +
scale_x_discrete(limits=1:8, labels = c(1:8))
plot_grid(CV_plot, OOB_plot, labels = "", ncol = 2, nrow = 1)
print(paste(paste0("Best CV error: ", min(err_matrix[,1]), "."), "Best value of m:", which.min(err_matrix[,1])))
print(paste(paste0("Best OOB error: ", min(err_matrix[,2]), "."), "Best value of m:", which.min(err_matrix[,2])))
set.seed(1)
rf_adj <- randomForest(x_train,
y_train,
xtest = x_test,
ytest = y_test,
mtry = 5,
importance = TRUE)
vip(rf_adj, aesthetics = c(fill = "blue")) +
theme_light()
ggplot(NULL, aes(x=c(1:500), y=rf_adj$test$mse)) +
theme_light() +
geom_line() +
labs(y="Test set MSE",
x="Number of Trees")
y_hat_rf <- rf_adj$test$predicted
MSE_rf <-compute_mse(y_hat_rf,test_data$lpsa) # calculate MSE
paste("MSE: ", MSE_rf)
paste("Squared root of the MSE: ",sqrt(MSE_rf))
set.seed(1)
matrix_boost <- matrix(0, nrow=8, ncol = 3)
for(d in 1:8){
boost <- gbm(lpsa ~ .,
data = df_prostate,
distribution = "gaussian",
n.trees = 1000,
interaction.depth = d,
cv.folds = 5,
verbose = F,
n.cores = 4)
matrix_boost[d,1] <- min(boost$cv.error)
matrix_boost[d,2] <- which.min(boost$cv.error)
matrix_boost[d,3] <- gbm.perf(boost,
plot.it=F,
oobag.curve=F,
method="OOB")
}
knitr::opts_chunk$set(warning=FALSE,
message=FALSE,
tidy.opts=list(width.cutoff = 60),
tidy = TRUE,
#fig.width = unit(3, "cm"),
#fig.height = unit(2, "cm"),
fig.align = "center"
)
library(tidyverse)
library(caret)
library(tree)
library(randomForest)
library(cowplot) # to use plot_grid
library(vip) # variable importance plot
library(gbm)
# Set working directory
setwd("~/Documents/Repo_Git/SL_homeworks/Homework2")
set.seed(1)
matrix_boost <- matrix(0, nrow=8, ncol = 3)
for(d in 1:8){
boost <- gbm(lpsa ~ .,
data = df_prostate,
distribution = "gaussian",
n.trees = 1000,
interaction.depth = d,
cv.folds = 5,
verbose = F,
n.cores = 4)
matrix_boost[d,1] <- min(boost$cv.error)
matrix_boost[d,2] <- which.min(boost$cv.error)
matrix_boost[d,3] <- gbm.perf(boost,
plot.it=F,
oobag.curve=F,
method="OOB")
}
print(paste("The cross validated MSE suggests",
matrix_boost[which.min(matrix_boost[,1]),2],
"iterations (trees) and a depth of",
which.min(matrix_boost[,1])))
print(paste("The OOB error suggests",
matrix_boost[which.min(matrix_boost[,1]),3],
"iterations (trees) and a depth of",
which.min(matrix_boost[which.min(matrix_boost[,1]),2])))
?gbm
set.seed(1)
matrix_boost <- matrix(0, nrow=8, ncol = 3)
for(d in 1:8){
boost <- gbm(lpsa ~ .,
data = df_prostate,
distribution = "gaussian",
n.trees = 1000,
interaction.depth = d,
cv.folds = 5,
verbose = F,
n.cores = 4)
matrix_boost[d,1] <- min(boost$cv.error)
matrix_boost[d,2] <- which.min(boost$cv.error)
matrix_boost[d,3] <- gbm.perf(boost,
plot.it=F,
oobag.curve=F,
method="OOB")
}
print(paste("The cross validated MSE suggests",
matrix_boost[which.min(matrix_boost[,1]),2],
"iterations (trees) and a depth of",
which.min(matrix_boost[,1])))
print(paste("The OOB error suggests",
matrix_boost[which.min(matrix_boost[,1]),3],
"iterations (trees) and a depth of",
which.min(matrix_boost[which.min(matrix_boost[,1]),2])))
set.seed(1)
matrix_boost <- matrix(0, nrow=8, ncol = 3)
for(d in 1:8){
boost <- gbm(lpsa ~ .,
data = df_prostate,
distribution = "gaussian",
n.trees = 1000,
interaction.depth = d,
cv.folds = 5,
verbose = F,
n.cores = 4)
matrix_boost[d,1] <- min(boost$cv.error)
matrix_boost[d,2] <- which.min(boost$cv.error)
matrix_boost[d,3] <- gbm.perf(boost,
plot.it=F,
oobag.curve=F,
method="OOB")
}
paste("The cross validated MSE suggests",
matrix_boost[which.min(matrix_boost[,1]),2],
"iterations (trees) and a depth of",
which.min(matrix_boost[,1]))
print(paste("The OOB error suggests",
matrix_boost[which.min(matrix_boost[,1]),3],
"iterations (trees) and a depth of",
which.min(matrix_boost[which.min(matrix_boost[,1]),2])))
paste("The cross validated MSE suggests",
matrix_boost[which.min(matrix_boost[,1]),2],
"iterations (trees) and a depth of",
which.min(matrix_boost[,1]))
paste("The OOB error suggests",
matrix_boost[which.min(matrix_boost[,1]),3],
"iterations (trees) and a depth of",
which.min(matrix_boost[which.min(matrix_boost[,1]),2]))
colnames(matrix_boost) <- c("MSE|", "Iterations (MSE)|", "Iterations (OOB)")
matrix_boost
colnames(matrix_boost) <- c("MSE|", "Iterations (MSE)|", "Iterations (OOB)")
knitr::kable(matrix_boost)
knitr::opts_chunk$set(warning=FALSE,
message=FALSE,
tidy.opts=list(width.cutoff = 60),
tidy = TRUE,
#fig.width = unit(3, "cm"),
#fig.height = unit(2, "cm"),
fig.align = "center"
)
library(tidyverse)
library(caret)
library(tree)
library(randomForest)
library(cowplot) # to use plot_grid
library(vip) # variable importance plot
library(gbm)
# Set working directory
setwd("~/Documents/Repo_Git/SL_homeworks/Homework2")
# Set seed
set.seed(1)
matrix_boost[1,2] <- gbm.perf(boost,
plot.it=F,
oobag.curve=F,
method="OOB")
mat_boost[1,3] <- gbm.perf(boost, method="cv")
matrix_boost[1,1] <- min(boost$cv.error)
matrix_boost[1,2] <- gbm.perf(boost,
plot.it=F,
oobag.curve=F,
method="OOB")
set.seed(1)
matrix_boost <- matrix(0, nrow=2, ncol = 3)
boosted <- gbm(lpsa ~ .,
data = df_prostate,
distribution = "gaussian",
n.trees = 1000,
interaction.depth = 6,
cv.folds = 5,
verbose = F,
n.cores = 4)
matrix_boost[1,1] <- min(boost$cv.error)
matrix_boost[1,2] <- gbm.perf(boost,
plot.it=F,
oobag.curve=F,
method="OOB")
matrix_boost[1,3] <- gbm.perf(boost, method="cv")
set.seed(1)
matrix_boost <- matrix(0, nrow=2, ncol = 3)
boosted <- gbm(lpsa ~ .,
data = df_prostate,
distribution = "gaussian",
n.trees = 1000,
interaction.depth = 6,
cv.folds = 5,
verbose = F,
n.cores = 4)
matrix_boost[1,1] <- min(boosted$cv.error)
matrix_boost[1,2] <- gbm.perf(boosted,
plot.it=F,
oobag.curve=F,
method="OOB")
matrix_boost[1,3] <- gbm.perf(boosted, method="cv")
knitr::opts_chunk$set(warning=FALSE,
message=FALSE,
tidy.opts=list(width.cutoff = 60),
tidy = TRUE,
#fig.width = unit(3, "cm"),
#fig.height = unit(2, "cm"),
fig.align = "center"
)
library(tidyverse)
library(caret)
library(tree)
library(randomForest)
library(cowplot) # to use plot_grid
library(vip) # variable importance plot
library(gbm)
# Set working directory
setwd("~/Documents/Repo_Git/SL_homeworks/Homework2")
colnames(matrix_boost) <- c("MSE|", "Iterations (MSE)|", "Iterations (OOB)")
matrix_boost
knitr::kable(matrix_boost)
knitr::opts_chunk$set(warning=FALSE,
message=FALSE,
tidy.opts=list(width.cutoff = 60),
tidy = TRUE,
#fig.width = unit(3, "cm"),
#fig.height = unit(2, "cm"),
fig.align = "center"
)
library(tidyverse)
library(caret)
library(tree)
library(randomForest)
library(cowplot) # to use plot_grid
library(vip) # variable importance plot
library(gbm)
# Set working directory
setwd("~/Documents/Repo_Git/SL_homeworks/Homework2")
df_prostate <- read.csv("./prostate.csv", sep = ",")
summary(df_prostate)
knitr::kable(head(df_prostate))
dim(df_prostate) # 97  9
set.seed(1)
sample <- caret::createDataPartition(df_prostate$lpsa, p=0.8)
train_data <- df_prostate[sample$Resample1, ]
test_data <- df_prostate[-sample$Resample1, ]
x_train <- model.matrix(lpsa ~ ., data = train_data)[, -1]
x_test <- model.matrix(lpsa ~ ., data = test_data)[, -1]
y_train <- train_data$lpsa
y_test <- test_data$lpsa
# Fit the model on the whole data
tree_prostate <- tree(lpsa ~ ., df_prostate)
summary(tree_prostate)
plot(tree_prostate) # plot the branches
text(tree_prostate, pretty = 0) # add the labels to the branches
title(main = "Unpruned tree")
tree_prostate
set.seed(1)
cv_tree <- cv.tree(object=tree_prostate)
graph <- tibble(dev=cv_tree$dev, tsize=cv_tree$size, alpha = cv_tree$k)
ggplot(data = graph, aes(x=tsize, y=dev)) +
geom_line(color="darkgrey") +
geom_point(color="black", size=3) +
geom_point(data = NULL,
aes(x=cv_tree$size[which.min(cv_tree$dev)],
y=min(cv_tree$dev),
colour="red",
size = 3)) +
labs(y = "Deviance", x = "Tree size") +
guides(size = "none", colour = "none") +
scale_x_discrete(labels= 1:9, limits= 1:9)
cv_tree
names(cv_tree) #  "size"   "dev"    "k"      "method"
cv_tree$size[which.min(cv_tree$dev)] # 4
opt.size <- cv_tree$size[which.min(cv_tree$dev)] # optimal size = 4
pruned_prostate <- prune.tree(tree_prostate, best = opt.size) # prune using the optimal size
# plot the pruned tree
plot(pruned_prostate)
text(pruned_prostate, pretty=0)
title(main="Pruned tree")
summary(pruned_prostate)
set.seed(1)
tree_prostate <- tree(lpsa ~ ., train_data)
pruned_prostate <- prune.tree(tree_prostate, best=4)
y_hat <- predict(pruned_prostate, test_data)
compute_mse <- function(preds, truth){mean((preds - truth)^2)}
MSE <- compute_mse(y_hat, test_data$lpsa) # calculate MSE
paste("MSE: ", MSE)
paste("Squared root of the MSE: ",sqrt(MSE))
set.seed(1)
nvar <- ncol(df_prostate) - 1 # number of predictors
m <- seq(1, nvar)
# initialize empty matrix that will contains the errors
err_matrix <- matrix(nrow = nvar, ncol = 2)
folds <- createFolds(df_prostate$lpsa, k = 5) # 5 folds
MSE <- c() # initialize empty list to store MSE
OOB <- c() # initialize empty list to store OOB error
for(i in m){ # iterate over m
for(f in folds){ # iterate over the folds
rf <- randomForest(lpsa ~ .,
data = df_prostate[-f, ],
mtry = i,
importance = TRUE)
y_hat_rf <- predict(rf, newdata=df_prostate[f, ])
MSE <- c(MSE, compute_mse(y_hat_rf,df_prostate[f, ]$lpsa))
OOB <- c(OOB, compute_mse(rf$predicted, df_prostate[-f,]$lpsa))
}
err_matrix[i, 1] <- mean(MSE)
err_matrix[i, 2] <- mean(OOB)
MSE <- c()
OOB <- c()
}
err_matrix
# convert matrix to df
err_df = as.data.frame(err_matrix, row.names=m)
colnames(err_df) = c("CV_error", "OOB_error")
# order results based on MSE
knitr::kable(err_df[order(err_df$CV_error),])
# order results based on OOB error
knitr::kable(err_df[order(err_df$OOB_error),])
CV_plot <- ggplot(NULL, aes(x=c(1:8), y=err_matrix[,1])) +
theme_light() +
geom_line(color="darkgrey") +
geom_point(color="black", size=3) +
geom_point(data = NULL, aes(x=c(1:8)[which.min(err_matrix[,1])],
y=min(err_matrix[,1]),
colour="red",
size = 3)) +
labs(y="CV error", x="m") +
guides(size = F, colour = F) +
scale_x_discrete(limits=1:8, labels = c(1:8))
OOB_plot <- ggplot(NULL, aes(x=c(1:8), y=err_matrix[,2])) +
theme_light() +
geom_line(color="darkgrey") +
geom_point(color="black", size=3) +
geom_point(data = NULL, aes(x=c(1:8)[which.min(err_matrix[,2])],
y=min(err_matrix[,2]),
color="red",
size = 3)) +
labs(y="OOB error", x="m") +
guides(size = F, colour = F) +
scale_x_discrete(limits=1:8, labels = c(1:8))
plot_grid(CV_plot, OOB_plot, labels = "", ncol = 2, nrow = 1)
print(paste(paste0("Best CV error: ", min(err_matrix[,1]), "."), "Best value of m:", which.min(err_matrix[,1])))
print(paste(paste0("Best OOB error: ", min(err_matrix[,2]), "."), "Best value of m:", which.min(err_matrix[,2])))
set.seed(1)
rf_adj <- randomForest(x_train,
y_train,
xtest = x_test,
ytest = y_test,
mtry = 5,
importance = TRUE)
vip(rf_adj, aesthetics = c(fill = "blue")) +
theme_light()
ggplot(NULL, aes(x=c(1:500), y=rf_adj$test$mse)) +
theme_light() +
geom_line() +
labs(y="Test set MSE",
x="Number of Trees")
y_hat_rf <- rf_adj$test$predicted
MSE_rf <-compute_mse(y_hat_rf,test_data$lpsa)
paste("MSE: ", MSE_rf)
paste("Squared root of the MSE: ",sqrt(MSE_rf))
set.seed(1)
matrix_boost <- matrix(0, nrow=8, ncol = 3)
for(d in 1:8){
boost <- gbm(lpsa ~ .,
data = df_prostate,
distribution = "gaussian",
n.trees = 1000,
interaction.depth = d,
cv.folds = 5,
verbose = F,
n.cores = 4)
matrix_boost[d,1] <- min(boost$cv.error)
matrix_boost[d,2] <- which.min(boost$cv.error)
matrix_boost[d,3] <- gbm.perf(boost,
plot.it=F,
oobag.curve=F,
method="OOB")
}
paste("The cross validated MSE suggests",
matrix_boost[which.min(matrix_boost[,1]),2],
"iterations (trees) and a depth of",
which.min(matrix_boost[,1]))
paste("The OOB error suggests",
matrix_boost[which.min(matrix_boost[,1]),3],
"iterations (trees) and a depth of",
which.min(matrix_boost[which.min(matrix_boost[,1]),2]))
colnames(matrix_boost) <- c("MSE|", "Iterations (MSE)|", "Iterations (OOB)")
matrix_boost
knitr::kable(matrix_boost)
length(matrix_boost)
dim(matrix_boost)
colnames(matrix_boost) <- c("MSE|", "Iterations (MSE)|", "Iterations (OOB)")
rownames(matrix_boost) <- seq(1:8)
matrix_boost
knitr::kable(matrix_boost)
colnames(matrix_boost) <- c("MSE|", "Iterations (MSE)|", "Iterations (OOB)")
matrix_boost
knitr::kable(matrix_boost)
colnames(matrix_boost) <- c("MSE|", "Iterations (MSE)|", "Iterations (OOB)")
knitr::kable(matrix_boost)
colnames(matrix_boost) <- c("MSE|", "Iterations (MSE)|", "Iterations (OOB)")
knitr::kable(matrix_boost)
set.seed(1)
matrix_boost2 <- matrix(0, nrow=2, ncol = 3)
boosted <- gbm(lpsa ~ .,
data = df_prostate,
distribution = "gaussian",
n.trees = 1000,
interaction.depth = 6,
cv.folds = 5,
verbose = F,
n.cores = 4)
summary(boosted)
matrix_boost2[1,1] <- min(boosted$cv.error)
matrix_boost2[1,2] <- gbm.perf(boosted,
plot.it=F,
oobag.curve=F,
method="OOB")
matrix_boost2[1,3] <- gbm.perf(boosted, method="cv")
set.seed(1)
matrix_boost2 <- matrix(0, nrow=2, ncol = 3)
boosted <- gbm(lpsa ~ .,
data = df_prostate,
distribution = "gaussian",
